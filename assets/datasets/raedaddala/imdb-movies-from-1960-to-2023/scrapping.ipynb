{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crawl IMdB Website for TOP Grossing Movies and their info from each year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install selenium\n",
        "%pip install bs4\n",
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.edge.service import Service\n",
        "from selenium.webdriver.edge.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_extract(soup_obj, selector, attribute=None, processing=None):\n",
        "    try:\n",
        "        if isinstance(soup_obj, (int, str)) or soup_obj is None:\n",
        "            return None\n",
        "        element = soup_obj.select_one(selector) if isinstance(selector, str) else soup_obj.find(*selector)\n",
        "        if element:\n",
        "            text = element.get(attribute) if attribute else element.text\n",
        "            return processing(text) if processing else text\n",
        "    except Exception as e:\n",
        "        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_awards(awards_text):\n",
        "    \"\"\"\n",
        "    Parse awards text to extract wins, nominations, and oscars.\n",
        "    Returns a dictionary with the counts.\n",
        "    \"\"\"\n",
        "    awards = {\n",
        "        \"wins\": 0,\n",
        "        \"nominations\": 0,\n",
        "        \"oscars\": 0\n",
        "    }\n",
        "    \n",
        "    if not awards_text:\n",
        "        return awards\n",
        "        \n",
        "    try:\n",
        "        # Extract Oscars\n",
        "        oscar_match = awards_text.lower().split('nominated for')\n",
        "        if oscar_match and len(oscar_match) > 1:\n",
        "            oscar_text = oscar_match[1].split()[0]\n",
        "            if oscar_text.isdigit():\n",
        "                awards[\"oscars\"] = int(oscar_text)\n",
        "        \n",
        "        # Extract wins and nominations\n",
        "        if '&' in awards_text:\n",
        "            parts = awards_text.lower().split('&')\n",
        "            \n",
        "            # Extract wins\n",
        "            wins_text = parts[0].split('wins')[0].strip().split()[-1]\n",
        "            if wins_text.isdigit():\n",
        "                awards[\"wins\"] = int(wins_text)\n",
        "            \n",
        "            # Extract nominations\n",
        "            noms_text = parts[1].split('nominations')[0].strip().split()[-1]\n",
        "            if noms_text.isdigit():\n",
        "                awards[\"nominations\"] = int(noms_text)\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing awards: {str(e)}\")\n",
        "        \n",
        "    return awards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_box_office_data(soup, test_id):\n",
        "    try:\n",
        "        element = soup.find(\"li\", {\"data-testid\": f\"title-boxoffice-{test_id}\"})\n",
        "        if element:\n",
        "            content = element.find(\"span\", class_=\"ipc-metadata-list-item__list-content-item\")\n",
        "            if content and content.text:\n",
        "                value = content.text.strip()\n",
        "                # Extract only the number, removing currency symbol and commas\n",
        "                amount = ''.join(c for c in value if c.isdigit())\n",
        "                return int(amount) if amount else None\n",
        "    except:\n",
        "        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_credits(soup):\n",
        "    credits = {\"directors\": [], \"writers\": [], \"stars\": []}\n",
        "\n",
        "    try:\n",
        "        # Find all principal credits\n",
        "        credits_section = soup.find(\"div\", {\"class\": \"sc-70a366cc-2\"})\n",
        "        if not credits_section:\n",
        "            return credits\n",
        "\n",
        "        credit_items = credits_section.find_all(\n",
        "            \"li\", {\"data-testid\": \"title-pc-principal-credit\"}\n",
        "        )\n",
        "\n",
        "        for item in credit_items:\n",
        "            # Get the label (Director/Writers/Stars)\n",
        "            label = item.find(\n",
        "                \"span\", {\"class\": \"ipc-metadata-list-item__label\"}\n",
        "            ) or item.find(\"a\", {\"class\": \"ipc-metadata-list-item__label\"})\n",
        "\n",
        "            if not label:\n",
        "                continue\n",
        "\n",
        "            label_text = label.text.lower().strip()\n",
        "\n",
        "            # Extract names based on the label\n",
        "            names = item.select(\"a.ipc-metadata-list-item__list-content-item--link\")\n",
        "            extracted_names = [name.text.strip() for name in names if name.text.strip()]\n",
        "\n",
        "            if \"director\" in label_text:\n",
        "                credits[\"directors\"] = extracted_names\n",
        "            elif \"writer\" in label_text:\n",
        "                credits[\"writers\"] = extracted_names\n",
        "            elif \"star\" in label_text:\n",
        "                credits[\"stars\"] = extracted_names\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting credits: {str(e)}\")\n",
        "\n",
        "    return credits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_list_data(soup, selector, class_name=None):\n",
        "    try:\n",
        "        if class_name:\n",
        "            elements = soup.select(f\"{selector} a.{class_name}\")\n",
        "        else:\n",
        "            elements = soup.select(f\"{selector} a.ipc-metadata-list-item__list-content-item--link\")\n",
        "        return [elem.text.strip() for elem in elements if elem.text.strip()]\n",
        "    except:\n",
        "        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_more_items(driver):\n",
        "    try:\n",
        "        # Updated XPath for the \"Load more\" button\n",
        "        button_xpath = \"//button[contains(@class, 'ipc-btn') and contains(@class, 'ipc-see-more')]\"\n",
        "        \n",
        "        # Wait for the button to be clickable\n",
        "        load_more = WebDriverWait(driver, 2).until(\n",
        "            EC.element_to_be_clickable((By.XPATH, button_xpath))\n",
        "        )\n",
        "        \n",
        "        # Scroll to button\n",
        "        driver.execute_script(\"arguments[0].scrollIntoView({ behavior: 'smooth', block: 'center' });\", load_more)\n",
        "        time.sleep(0.5)\n",
        "        \n",
        "        # Click the button\n",
        "        load_more.click()\n",
        "        time.sleep(2)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Load more error: {str(e)}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_genres(soup):\n",
        "    try:\n",
        "        genre_list = soup.select(\"div.ipc-chip-list__scroller a.ipc-chip--on-baseAlt span.ipc-chip__text\")\n",
        "        return [genre.text.strip() for genre in genre_list if genre.text.strip()]\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crawl_imdb_movies(year: int):\n",
        "    output_dir = os.path.join(\"Data\", str(year))\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    url = f\"https://www.imdb.com/search/title/?title_type=feature&release_date={year}-01-01,{year}-12-31&count=100&sort=boxoffice_gross_us,desc\"\n",
        "\n",
        "    options = Options()\n",
        "    options.add_argument(\"--lang=en-US\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--window-size=1920,1080\")\n",
        "\n",
        "    driver_path = \"edgedriver.exe\"\n",
        "    service = Service(executable_path=driver_path)\n",
        "    driver = webdriver.Edge(service=service, options=options)\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Wait for the movie list to be present\n",
        "        movie_list_xpath = \"//ul[contains(@class, 'ipc-metadata-list')]\"\n",
        "        WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_element_located((By.XPATH, movie_list_xpath))\n",
        "        )\n",
        "\n",
        "        # Load more movies\n",
        "        loaded_data = 100\n",
        "        while loaded_data < 600:\n",
        "            if load_more_items(driver):\n",
        "                loaded_data += 100\n",
        "            else:\n",
        "                print(\"Failed to load more items\")\n",
        "                break\n",
        "\n",
        "        # Parse the page content\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "        # Find the movie list container\n",
        "        movie_list = soup.find(\"ul\", class_=\"ipc-metadata-list\")\n",
        "        if not movie_list:\n",
        "            print(\"Movie list container not found!\")\n",
        "            return None\n",
        "\n",
        "        # Find all movie items\n",
        "        film_containers = movie_list.find_all(\n",
        "            \"li\", class_=\"ipc-metadata-list-summary-item\"\n",
        "        )\n",
        "\n",
        "        films_data = []\n",
        "        for film in film_containers:\n",
        "            try:\n",
        "                # Extract title\n",
        "                title_element = film.find(\"h3\", class_=\"ipc-title__text\")\n",
        "                title = title_element.text.strip() if title_element else None\n",
        "\n",
        "                # Extract movie link\n",
        "                link_element = film.find(\"a\", class_=\"ipc-title-link-wrapper\")\n",
        "                movie_link = (\n",
        "                    f\"https://www.imdb.com{link_element['href']}\"\n",
        "                    if link_element and \"href\" in link_element.attrs\n",
        "                    else None\n",
        "                )\n",
        "\n",
        "                # Extract metadata\n",
        "                metadata = film.find_all(\"span\", class_=\"dli-title-metadata-item\")\n",
        "                year_text = metadata[0].text if len(metadata) > 0 else None\n",
        "                duration = metadata[1].text if len(metadata) > 1 else None\n",
        "                mpa = metadata[2].text if len(metadata) > 2 else None\n",
        "\n",
        "                # Extract rating\n",
        "                rating_element = film.find(\"span\", class_=\"ipc-rating-star--rating\")\n",
        "                rating = rating_element.text.strip() if rating_element else None\n",
        "\n",
        "                # Extract votes\n",
        "                votes_element = film.find(\"span\", class_=\"ipc-rating-star--voteCount\")\n",
        "                votes = (\n",
        "                    votes_element.text.replace(\" \", \"\").strip(\"()\")[2::]\n",
        "                    if votes_element\n",
        "                    else None\n",
        "                )\n",
        "                film_data = {\n",
        "                    \"Title\": title,\n",
        "                    \"Movie Link\": movie_link,\n",
        "                    \"Year\": year_text,\n",
        "                    \"Duration\": duration,\n",
        "                    \"MPA\": mpa,\n",
        "                    \"Rating\": rating,\n",
        "                    \"Votes\": votes,\n",
        "                }\n",
        "                films_data.append(film_data)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing film: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if not films_data:\n",
        "            print(\"No movies data was collected!\")\n",
        "            return None\n",
        "\n",
        "        # Create DataFrame and save initial data\n",
        "        initial_movies_df = pd.DataFrame(films_data)\n",
        "        initial_movies_path = os.path.join(output_dir, f\"imdb_movies_{year}.csv\")\n",
        "        initial_movies_df.to_csv(initial_movies_path, index=False)\n",
        "\n",
        "\n",
        "        if (\n",
        "            \"Movie Link\" not in initial_movies_df.columns\n",
        "            or initial_movies_df[\"Movie Link\"].isna().all()\n",
        "        ):\n",
        "            return initial_movies_df\n",
        "\n",
        "        # Extract advanced movie details\n",
        "        all_movie_data = []\n",
        "        for url in initial_movies_df[\"Movie Link\"].dropna():\n",
        "            try:\n",
        "                driver.get(url)\n",
        "                time.sleep(1)\n",
        "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "                # Extract all credits at once\n",
        "                credits = extract_credits(soup)\n",
        "\n",
        "                advanced_details = {\n",
        "                    \"Movie Link\": url,\n",
        "                    \"budget\": extract_box_office_data(soup, \"budget\"),\n",
        "                    \"grossWorldWide\": extract_box_office_data(\n",
        "                        soup, \"cumulativeworldwidegross\"\n",
        "                    ),\n",
        "                    \"gross_US_Canada\": extract_box_office_data(soup, \"grossdomestic\"),\n",
        "                    \"opening_weekend_Gross\": extract_box_office_data(\n",
        "                        soup, \"openingweekenddomestic\"\n",
        "                    ),\n",
        "                    \"directors\": credits[\"directors\"],\n",
        "                    \"writers\": credits[\"writers\"],\n",
        "                    \"stars\": credits[\"stars\"],\n",
        "                    \"genres\": extract_genres(soup),\n",
        "                    \"countries_origin\": extract_list_data(\n",
        "                        soup, \"li[data-testid='title-details-origin']\"\n",
        "                    ),\n",
        "                    \"filming_locations\": extract_list_data(\n",
        "                        soup, \"li[data-testid='title-details-filminglocations']\"\n",
        "                    ),\n",
        "                    \"production_companies\": extract_list_data(\n",
        "                        soup, \"li[data-testid='title-details-companies']\"\n",
        "                    ),\n",
        "                    \"Languages\": extract_list_data(\n",
        "                        soup, \"li[data-testid='title-details-languages']\"\n",
        "                    ),\n",
        "                }\n",
        "\n",
        "                # Extract awards data\n",
        "                awards_element = soup.find(\"li\", {\"data-testid\": \"award_information\"})\n",
        "                if awards_element:\n",
        "                    awards_text = awards_element.text.strip()\n",
        "                    awards_dict = parse_awards(awards_text)\n",
        "                    advanced_details.update(awards_dict)\n",
        "                else:\n",
        "                    advanced_details.update({\"wins\": 0, \"nominations\": 0, \"oscars\": 0})\n",
        "\n",
        "                advanced_details[\"release_date\"] = safe_extract(\n",
        "                    soup, \"a[href*='releaseinfo']\"\n",
        "                )\n",
        "\n",
        "                all_movie_data.append(advanced_details)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {url}: {str(e)}\")\n",
        "                all_movie_data.append({\"Movie Link\": url})\n",
        "\n",
        "        advanced_movies_df = pd.DataFrame(all_movie_data)\n",
        "        advanced_movies_path = os.path.join(\n",
        "            output_dir, f\"advanced_movies_details_{year}.csv\"\n",
        "        )\n",
        "        advanced_movies_df.to_csv(advanced_movies_path, index=False)\n",
        "\n",
        "        merged_data = pd.merge(\n",
        "            initial_movies_df, advanced_movies_df, how=\"left\", on=\"Movie Link\"\n",
        "        )\n",
        "        merged_path = os.path.join(output_dir, f\"merged_movies_data_{year}.csv\")\n",
        "        merged_data.to_csv(merged_path, index=False)\n",
        "\n",
        "        return merged_data\n",
        "\n",
        "    finally:\n",
        "        driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "years_to_crawl = range(1960, 2025)\n",
        "for year in years_to_crawl:\n",
        "    print(f\"Crawling data for year {year}\")\n",
        "    crawl_imdb_movies(year)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
